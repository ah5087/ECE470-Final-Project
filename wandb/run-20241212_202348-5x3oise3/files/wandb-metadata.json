{
  "os": "Linux-6.1.85+-x86_64-with-glibc2.35",
  "python": "3.10.12",
  "startedAt": "2024-12-12T20:23:48.241461Z",
  "args": [
    "train_llama3.json"
  ],
  "program": "/usr/local/bin/llamafactory-cli",
  "git": {
    "remote": "https://github.com/hiyouga/LLaMA-Factory.git",
    "commit": "2811814fc42fb214b3e8be1055f9f57ffd0ffb12"
  },
  "email": "ah5087@princeton.edu",
  "root": "/content/LLaMA-Factory",
  "host": "e0882ee2a306",
  "username": "root",
  "executable": "/usr/bin/python3",
  "cpu_count": 1,
  "cpu_count_logical": 2,
  "gpu": "Tesla T4",
  "gpu_count": 1,
  "disk": {
    "/": {
      "total": "120942624768",
      "used": "49472753664"
    }
  },
  "memory": {
    "total": "13609431040"
  },
  "cpu": {
    "count": 1,
    "countLogical": 2
  },
  "gpu_nvidia": [
    {
      "name": "Tesla T4",
      "memoryTotal": "16106127360",
      "cudaCores": 2560,
      "architecture": "Turing"
    }
  ],
  "cudaVersion": "12.2"
}