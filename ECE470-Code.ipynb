{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Llama-3 with LLaMA Factory\n",
    "\n",
    "Please use a **free** Tesla T4 Colab GPU to run this!\n",
    "\n",
    "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "%rm -rf LLaMA-Factory\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "!pip uninstall -y jax\n",
    "!pip install -e .[torch,bitsandbytes,liger-kernel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation, Augmentation, and Splitting\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split  # CHANGED: Imported train_test_split\n",
    "import nltk  # ADDED: Importing nltk for data augmentation\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# ADDED: Download NLTK data files\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')  # FIXED: Downloading 'punkt' instead of 'punkt_tab'\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "%cd /content\n",
    "\n",
    "# Function to remove links from text\n",
    "def remove_links(text):\n",
    "    # Regular expression to match URLs\n",
    "    return re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "# ADDED: Function to perform synonym replacement for data augmentation\n",
    "def synonym_replacement(text, n=2):\n",
    "    words = nltk.word_tokenize(text)  # FIXED: Ensure 'punkt' is downloaded for word_tokenize\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()  # Get the first synonym\n",
    "            if synonym != random_word:\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = 'COS-ECE-470-fa-2024-Finalized-Dataset.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select the relevant columns for task description and desired solution\n",
    "task_columns = df[['Describe your task.',\n",
    "                  'Desired solution to your task:  Provide a solution that clearly explains the rationale and logic of solving the problem. The solution should be instructional, with reasoning steps for audience comprehension.\\n(Any format is acceptable, e.g. a link, attached file, or text). ']]\n",
    "\n",
    "# ADDED: Apply data augmentation by paraphrasing\n",
    "augmented_tasks = []\n",
    "augmented_solutions = []\n",
    "\n",
    "for _, row in task_columns.iterrows():\n",
    "    task = row['Describe your task.'] if pd.notna(row['Describe your task.']) else \"No task description provided.\"\n",
    "    solution = row['Desired solution to your task:  Provide a solution that clearly explains the rationale and logic of solving the problem. The solution should be instructional, with reasoning steps for audience comprehension.\\n(Any format is acceptable, e.g. a link, attached file, or text). ']\n",
    "    solution = remove_links(solution) if pd.notna(solution) else \"No solution provided.\"\n",
    "    \n",
    "    # Original data\n",
    "    augmented_tasks.append(task)\n",
    "    augmented_solutions.append(solution)\n",
    "    \n",
    "    # Augmented data\n",
    "    augmented_tasks.append(synonym_replacement(task))\n",
    "    augmented_solutions.append(synonym_replacement(solution))\n",
    "\n",
    "augmented_df = pd.DataFrame({\n",
    "    'Describe your task.': augmented_tasks,\n",
    "    'Desired solution to your task:  Provide a solution that clearly explains the rationale and logic of solving the problem. The solution should be instructional, with reasoning steps for audience comprehension.\\n(Any format is acceptable, e.g. a link, attached file, or text). ': augmented_solutions\n",
    "})\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% val)\n",
    "train_df, val_df = train_test_split(augmented_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# CHANGED: Function to prepare ShareGPT format data remains the same\n",
    "def prepare_sharegpt_data(dataframe):\n",
    "    sharegpt_data = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Handle missing task description\n",
    "        user_content = row['Describe your task.'] if pd.notna(row['Describe your task.']) else \"No task description provided.\"\n",
    "\n",
    "        # Clean and handle desired solution\n",
    "        gpt_content = remove_links(row['Desired solution to your task:  Provide a solution that clearly explains the rationale and logic of solving the problem. The solution should be instructional, with reasoning steps for audience comprehension.\\n(Any format is acceptable, e.g. a link, attached file, or text). '])\n",
    "\n",
    "        # Provide default message if solution is empty\n",
    "        if not gpt_content.strip():\n",
    "            gpt_content = \"No solution provided.\"\n",
    "\n",
    "        # Create entry for ShareGPT format\n",
    "        entry = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": user_content\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": gpt_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Append only if both contents are valid\n",
    "        if user_content and gpt_content:\n",
    "            sharegpt_data.append(entry)\n",
    "    return sharegpt_data\n",
    "\n",
    "# CHANGED: Prepare training data\n",
    "train_sharegpt_data = prepare_sharegpt_data(train_df)\n",
    "\n",
    "# CHANGED: Prepare validation data\n",
    "val_sharegpt_data = prepare_sharegpt_data(val_df)\n",
    "\n",
    "# Change directory to LLaMA-Factory data folder\n",
    "%cd /content/LLaMA-Factory/data/\n",
    "\n",
    "# Save the prepared training data\n",
    "with open('ECE470_train_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save the prepared validation data\n",
    "with open('ECE470_val_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved the training dataset to 'ECE470_train_data.json' and validation dataset to 'ECE470_val_data.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",                        # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  do_eval=True,                        # Enable evaluation on validation set\n",
    "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"ECE470_train_data\",     # REPLACED: 'train_file' with 'dataset'\n",
    "  eval_dataset=\"ECE470_val_data\",  # Kept 'eval_dataset' as is\n",
    "  template=\"llama3\",                   # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",              # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                   # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"llama3_lora\",            # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=2,       # the batch size\n",
    "  gradient_accumulation_steps=4,       # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",          # use cosine learning rate scheduler\n",
    "  logging_steps=10,                    # log every 10 steps\n",
    "  warmup_ratio=0.1,                    # use warmup scheduler\n",
    "  save_steps=1000,                     # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                  # the learning rate\n",
    "  num_train_epochs=3.0,                # Increased epochs for better training\n",
    "  max_samples=500,                     # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                   # clip gradient norm to 1.0\n",
    "  loraplus_lr_ratio=16.0,              # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                           # use float16 mixed precision training\n",
    "  #use_liger_kernel=True,              # use liger kernel for efficient training\n",
    "  report_to=\"wandb\",                   # Report to Weights & Biases for visualization\n",
    ")\n",
    "\n",
    "# Optionally, specify dataset_dir if your datasets are in a different directory\n",
    "# args[\"dataset_dir\"] = \"data\"\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "!llamafactory-cli train train_llama3.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
